{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ea77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from simple_grid import simple_grid as gridworld\n",
    "from simple_grid_agent import GridworldAgent as Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4359ef54",
   "metadata": {},
   "source": [
    "Read through all the classes and functions defined inside `simple_grid` environment and `GridworldAgent` to familiarize yourself with the details of this assignment.\n",
    "\n",
    "Consider a simple gridworld where actions do not result in deterministic state changes. We specify that there is a $20\\%$ probability that the selected action would result in a stochastic state transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d4db67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7df4d",
   "metadata": {},
   "source": [
    "The following set of commands will help you familiarize with different components of the gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67209706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Reward For each Tile \n",
      "\n",
      "\n",
      "----------\n",
      "0 |0 |0 |\n",
      "----------\n",
      "0 |-5 |5 |\n",
      "----------\n",
      "0 |0 |0 |"
     ]
    }
   ],
   "source": [
    "print('\\n Reward For each Tile \\n')\n",
    "env.print_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe348607",
   "metadata": {},
   "source": [
    "Check out the set of possible actions for the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da25d71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Set of possible actions in numerical form. These are actual inputs to the gridworld agent \n",
      "\n",
      "[0 1 2 3]\n",
      "\n",
      " Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \n",
      "\n",
      "['U' 'L' 'D' 'R']\n"
     ]
    }
   ],
   "source": [
    "print('\\n Set of possible actions in numerical form. These are actual inputs to the gridworld agent \\n')\n",
    "print(env.action_space)\n",
    "\n",
    "print('\\n Set of possible actions in the grid in text form. They map 1 to 1 from numbers above to direction \\n')\n",
    "print(env.action_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c13240b",
   "metadata": {},
   "source": [
    "Consider a policy which tries to reach the goal state(+5) as fast as possible. Below we define the policy to evaluate the state values for this policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4333604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Policy: Fastest Path to Goal State(Does not take reward into consideration) \n",
      "\n",
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "print('\\n Policy: Fastest Path to Goal State(Does not take reward into consideration) \\n')\n",
    "a.print_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ac3f4",
   "metadata": {},
   "source": [
    "**Q1**\n",
    "\n",
    "Implement the `get_v` and `get_q` methods to estimate the state value and state-action value in `simple_grid_agent.py`. These may be used later on for debugging your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014e037",
   "metadata": {},
   "source": [
    "**Q2** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-value estimation equations inside `mc_predict_v` in `simple_grid_agent.py`.\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Prediction? Why?\n",
    "\n",
    "NB: assume anyvist and everyvisit to be interchangeable terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d22e763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State Values for first_visit MC state estimation \n",
      "\n",
      "\n",
      "---------------\n",
      "-0.7 |0.7 |2.8 |\n",
      "---------------\n",
      "-3.7 |1.9 |0 |\n",
      "---------------\n",
      "-4.0 |-3.4 |2.4 |\n",
      " State Values for any_visit MC state estimation \n",
      "\n",
      "\n",
      "---------------\n",
      "-0.5 |0.5 |1.6 |\n",
      "---------------\n",
      "-1.9 |1.1 |0 |\n",
      "---------------\n",
      "-2.3 |-1.9 |1.4 |"
     ]
    }
   ],
   "source": [
    " # evaluate state values for policy_fast for both first-vist and any-vist\n",
    "print('\\n State Values for first_visit MC state estimation \\n')\n",
    "a.mc_predict_v()\n",
    "a.print_v()\n",
    "\n",
    "print('\\n State Values for any_visit MC state estimation \\n')\n",
    "a.mc_predict_v(first_visit=False)\n",
    "a.print_v()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb48cd7",
   "metadata": {},
   "source": [
    "**Q3** \n",
    "\n",
    "The Monte Carlo rollout itself has been implemented in `simple_grid_agent.py` inside the `run_episode` method.\n",
    "\n",
    "**Implement** \n",
    "\n",
    "First-visit as well as any-visit Monte Carlo state-action value estimation equations inside `mc_predict_q` in `simple_grid_agent.py`\n",
    "These have been discussed in class. Refer to Sutton and Barto Chapter 5 for further details to implement them.\n",
    "\n",
    "Test and report inside this notebook the results using the following commands. Are there sufficient differences in the state values under anyvisit and firstvisit MC Q value Prediction? Why?\n",
    "\n",
    "**My Answer**\n",
    "\n",
    "The difference is large enough if the Agent is not reset, as it is continuously being overwritten and its not very clear. \n",
    "But, if we reset the Agent each time, we can observe that the first visit and any visit methods have almost similar values. This is due to values converging at *infinity* (very high values), and 10000 iterations seem to be large enough for the problem. Thus, it seems like both the MC methods reach near convergence, hence having similar values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14bc2881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " State action Values for first_visit MC state action estiamtion \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-3.90329919 -3.90329919 -3.90329919 -3.90329919]\n",
      "(2, 1) [-3.34172175 -3.34172175 -3.34172175 -3.34172175]\n",
      "(2, 2) [2.57401957 2.57401957 2.57401957 2.57401957]\n",
      "(1, 1) [1.96939246 1.96939246 1.96939246 1.96939246]\n",
      "(0, 1) [0.79994935 0.79994935 0.79994935 0.79994935]\n",
      "(0, 0) [-0.99826627 -0.99826627 -0.99826627 -0.99826627]\n",
      "(0, 2) [2.72404822 2.72404822 2.72404822 2.72404822]\n",
      "(1, 0) [-3.56870279 -3.56870279 -3.56870279 -3.56870279]\n",
      "(1, 2) [0. 0. 0. 0.]\n",
      "\n",
      " State action Values for any_visit MC state action estimation \n",
      "\n",
      "\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-3.96960571 -3.96960571 -3.96960571 -3.96960571]\n",
      "(1, 0) [-3.6943788 -3.6943788 -3.6943788 -3.6943788]\n",
      "(1, 1) [1.92188501 1.92188501 1.92188501 1.92188501]\n",
      "(0, 1) [0.80356688 0.80356688 0.80356688 0.80356688]\n",
      "(0, 2) [2.68179496 2.68179496 2.68179496 2.68179496]\n",
      "(2, 1) [-3.39222104 -3.39222104 -3.39222104 -3.39222104]\n",
      "(2, 2) [2.5771943 2.5771943 2.5771943 2.5771943]\n",
      "(0, 0) [-0.98638349 -0.98638349 -0.98638349 -0.98638349]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Resetting Agent for the first value method\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# evaluate state action values for policy_fast\n",
    "print('\\n State action Values for first_visit MC state action estiamtion \\n')\n",
    "a.mc_predict_q()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])\n",
    "\n",
    "#Resetting Agent for the any value method\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "            start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "    \n",
    "# evaluate state action values for policy_fast\n",
    "print('\\n State action Values for any_visit MC state action estimation \\n')\n",
    "a.mc_predict_q(first_visit=False)\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0166ff",
   "metadata": {},
   "source": [
    "**Q4**\n",
    "\n",
    "Now we implement Monte Carlo control using state-action values. \n",
    "\n",
    "**Implement**\n",
    "\n",
    "Complete the snippet in `mc_control_q` inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0cd6d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "U |U |U |\n",
      "----------\n",
      "U |U |U |\n",
      "----------\n",
      "U |U |U |\n",
      " Actions: {env.action_text} \n",
      "\n",
      "(2, 0) [-4.06428327 -4.06428327 -4.06428327 -4.06428327]\n",
      "(2, 1) [-3.61741734 -3.61741734 -3.61741734 -3.61741734]\n",
      "(1, 1) [1.83840261 1.83840261 1.83840261 1.83840261]\n",
      "(1, 0) [-4.09648402 -4.09648402 -4.09648402 -4.09648402]\n",
      "(0, 1) [0.00152062 0.00152062 0.00152062 0.00152062]\n",
      "(0, 0) [-2.77851795 -2.77851795 -2.77851795 -2.77851795]\n",
      "(0, 2) [2.76265248 2.76265248 2.76265248 2.76265248]\n",
      "(2, 2) [2.16922656 2.16922656 2.16922656 2.16922656]\n",
      "(1, 2) [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "# Run MC Control\n",
    "a.mc_control_q(n_episode = 1000,first_visit=False)\n",
    "a.print_policy()\n",
    "\n",
    "print('\\n Actions: {env.action_text} \\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e25c50",
   "metadata": {},
   "source": [
    "**Q5**\n",
    "\n",
    "Bonus!\n",
    "\n",
    "**Implement**\n",
    "\n",
    "Greedy within The Limit of  Iinfinite Exploration MC Control in `mc_control_glie` function inside `simple_grid_agent.py`\n",
    "\n",
    "Test and report inside this notebook the results using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "597e3cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "R |R |D |\n",
      "----------\n",
      "R |R |U |\n",
      "----------\n",
      "R |U |U |\n",
      " Actions ['U' 'L' 'D' 'R'] \n",
      "\n",
      "(2, 0) [-4.43721661 -4.7530688  -4.30619662 -3.81638594]\n",
      "(2, 1) [-3.46260196 -5.2509598  -4.58948807  0.47365521]\n",
      "(1, 1) [-0.61335505 -4.59878555 -4.0423211   3.42311793]\n",
      "(1, 0) [-1.5817005  -3.15445273 -3.70110562 -3.28002722]\n",
      "(2, 2) [ 3.24556312 -2.35457034  2.348       2.30352941]\n",
      "(0, 1) [-0.77147403 -2.32595881 -3.04926663  1.70161609]\n",
      "(0, 2) [ 0.59276    -0.70399232  3.42073377  1.30634498]\n",
      "(0, 0) [-1.93911085 -3.15227375 -2.038      -1.22695815]\n"
     ]
    }
   ],
   "source": [
    "#stochastic environment\n",
    "env = gridworld(wind_p=0.2)\n",
    "\n",
    "#initial policy\n",
    "policy_fast = {(0, 0): 3,\n",
    "          (0, 1): 3,\n",
    "          (0, 2): 2,\n",
    "          (1, 0): 3,\n",
    "          (1, 1): 3,\n",
    "          (1, 2): 0,\n",
    "          (2, 0): 3,\n",
    "          (2, 1): 0,\n",
    "          (2, 2): 0}\n",
    "\n",
    "#stochastic agent - epsilon greedy with decays\n",
    "a = Agent(env, policy = policy_fast, gamma = 0.9, \n",
    "        start_epsilon=0.9,end_epsilon=0.3,epsilon_decay=0.9)\n",
    "\n",
    "a.mc_control_glie(n_episode = 1000)\n",
    "a.print_policy()\n",
    "print('\\n Actions', env.action_text, '\\n')\n",
    "for i in a.q: print(i,a.q[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b783bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f3f71acbe028193bc0bf25c7bbd98f54ca429f192d17b6451f61294f7eafb542"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
